[{"content":" 课程介绍 统计研究院陳鄰安老师数理统计，参考网站：统计学1，统计学2;youtube课程：交大數理統計學，陳鄰安老師 - 高等統計學。\nRandom variables Define 1(random variable). Let $(\\Omega, \\mathcal{F}, P)$ be a probability space and $(E, \\mathcal{E})$ a measurable space. Then an $(E, \\mathcal{E})$-valued random variable is a measurable function $X: \\Omega \\rightarrow E$. Remark.\n随机变量是从样本空间到$\\mathbb{R}$的映射，概率测度是从$\\sigma$-域到$[0,1]$的映射，并且满足整个样本空间的测度等于1。在实际中我们一般不会考虑原来的样本空间，因为一般考虑由随机变量诱导出来的概率测度，它是由博雷尔集($\\mathcal{B}$)到$\\mathbb{R}$的映射。 $\\forall B\\in \\mathcal{B},X^{-1}(B)\\in \\mathcal{F},\\text{where} X^{-1}(B) = {\\omega \\in \\Omega\\mid X(\\omega)\\in B}$,which means that, for every subset $B \\in \\mathcal{E}$, its preimage is $\\mathcal{F}$-measurable; $X^{-1}(B) \\in \\mathcal{F}$, where $X^{-1}(B)={\\omega: X(\\omega) \\in B}$. This definition enables us to measure any subset $B \\in \\mathcal{E}$ in the target space by looking at its preimage, which by assumption is measurable. 由随机变量很容易定义累积分布函数(CDF)： $$ F(x) = P(X(\\omega) \\leq x) = P(\\{\\omega\\in\\Omega:X(\\omega)\\in (-\\infty,x]\\}) $$ Statistical Inference - Point Estimation Continue to Point Estimation-UMVUE Confidence Interval ","date":"2025-01-26T00:21:05+08:00","image":"https://example.com/post_file/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/cover.png","permalink":"https://example.com/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/","title":"学习笔记|数理统计"},{"content":" 课程介绍 谢启鸿老师高等代数学，参考网站：谢启鸿高等代数官方博客,b站课程：数学专业 高等代数学-复旦大学-谢启鸿-高清。\n第一章 行列式 可以由递归形式方便的定义n阶行列式，并且定义余子式和代数余子式。下面先介绍行列式的一些性质：\n性质1 上下三角行列式的值等于其对角线元素之积。\n性质2 行列式的某行或某列全为零，则行列式等于零。\n性质3 用常数$c$乘行列式的某一列或某一行，得到的行列式的值等于原行列式的$c$倍。\n性质4 交换行列式不同的两行(列)，行列式的值改变符号。\n性质5 若行列式两行或两列成比例，则行列式的值等于零。特别，若行列式两行或两列相同，则行列式的值等于零。\n性质6 若行列式中某行(列)元素均为两项之和，则行列式可以表示为两个行列式之和。\n性质7 行列式的某一行(列)乘以某个数加到另外一行(列)上，行列式的值不变。\n性质8 行列式和其转置具有相同的值。\n定理1 行列式可以按照任意一行或任意一列的形式进行展开，如果线性组合的行(列)和对应的余子式(代数余子式)不匹配，计算结果等于零。\n定理2(Cramer法则) 设有线性方程组\n$$ \\left\\{\\begin{array}{c} a_{11} x_1+a_{12} x_2+\\cdots+a_{1 n} x_n=b_1 \\\\ a_{21} x_1+a_{22} x_2+\\cdots+a_{2 n} x_n=b_2 \\\\ \\cdots \\cdots \\cdots \\\\ a_{n 1} x_1+a_{n 2} x_2+\\cdots+a_{n n} x_n=b_n \\end{array}\\right. $$ 记这个方程组的系数行列式为 $|\\boldsymbol{A}|$ ，若 $|\\boldsymbol{A}| \\neq 0$ ，则方程组有且仅有一组解： $$ x_1=\\frac{\\left|A_1\\right|}{|\\boldsymbol{A}|}, x_2=\\frac{\\left|A_2\\right|}{|\\boldsymbol{A}|}, \\cdots, x_n=\\frac{\\left|A_n\\right|}{|\\boldsymbol{A}|} $$ 其中 $\\left|\\boldsymbol{A}_{\\boldsymbol{j}}\\right|(j=1,2, \\cdots, n)$ 是一个 $n$ 阶行列式，它由 $|\\boldsymbol{A}|$ 去掉第 $j$ 列换上方程组的常数项 $b_1, b_2, \\cdots, b_n$ 组成的列而成． 例1 对于行列式的计算，优先利用性质将行列式进行降阶，构造出0和1。特殊的，对于Vandermonde行列式：\n$$ \\begin{aligned} V_n=\\left|\\begin{array}{ccccc} 1 \u0026 x_1 \u0026 x_1^2 \u0026 \\cdots \u0026 x_1^{n-1} \\\\ 1 \u0026 x_2 \u0026 x_2^2 \u0026 \\cdots \u0026 x_2^{n-1} \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \u0026 \\\\ 1 \u0026 x_n \u0026 x_n^2 \u0026 \\cdots \u0026 x_n^{n-1} \\end{array}\\right|=\\prod_{1 \\leqslant i \u003c j \\leqslant n}\\left(x_j-x_i\\right) \\end{aligned} $$ 计算降次的Vandermonde行列式时，将上式修改为 $(x_i-x_j)$。\n定义3(行列式的组合定义) 设方阵 $A=\\left(a_{i j}\\right) \\in M_n(\\mathbb{F})$ ，则 $A$ 的行列式定义为\n$$ |A|=\\sum_{\\left(i_1, i_2, \\cdots, i_n\\right) \\in S_n}(-1)^{N\\left(i_1, i_2, \\cdots, i_n\\right)} a_{i_1 1} a_{i_2 2} \\cdots a_{i_n n} $$\n其中 $S_n$ 是 ${1,2, \\cdots, n}$ 的所有全排列构成的集合， $N\\left(i_1, i_2, \\cdots, i_n\\right)$ 是全排列 $\\left(i_1, i_2, \\cdots, i_n\\right)$ 的逆序数 ，$a_{i_1 1} a_{i_2 2} \\cdots a_{i_n n}$ 称为行列式 $|A|$ 中的单项，它从 $A$ 的每行每列各取一个元素相乘得到， $(-1)^{N\\left(i_1, i_2, \\cdots, i_n\\right)}$ 称为这个单项的符号． 容易看出：当 $n \\geq 2$ 时，符号为 $\\pm 1$ 的单项各为一半，即 $\\frac{1}{2} n!$ 个．\n定理4 (Laplace 定理) 设 $|\\boldsymbol{A}|$ 是 $n$ 阶行列式，在 $|\\boldsymbol{A}|$ 中任取 $k$ 行（列），那么含于这 $k$ 行（列）的全部 $k$ 阶子式与它们所对应的代数余子式的乘积之和等于 $|A|$ ．即若取定 $k$ 个行： $1 \\leq i_1\u0026lt;i_2\u0026lt;\\cdots\u0026lt;i_k \\leq n$ ，则\n$$ \\begin{aligned} |\\boldsymbol{A}|=\\sum_{1 \\leq j_1\u0026lt;j_2\u0026lt;\\cdots\u0026lt;j_k \\leq n} \\boldsymbol{A}\\left(\\begin{array}{cccc} i_1 \u0026amp; i_2 \u0026amp; \\cdots \u0026amp; i_k; j_1 \u0026amp; j_2 \u0026amp; \\cdots \u0026amp; j_k \\end{array}\\right) \\widehat{\\boldsymbol{A}}\\left(\\begin{array}{cccc} i_1 \u0026amp; i_2 \u0026amp; \\cdots \u0026amp; i_k ; j_1 \u0026amp; j_2 \u0026amp; \\cdots \u0026amp; j_k \\end{array}\\right) . \\end{aligned} $$\n同样若取定 $k$ 个列： $1 \\leq j_1\u0026lt;j_2\u0026lt;\\cdots\u0026lt;j_k \\leq n$ ，则\n$$ \\begin{aligned} |\\boldsymbol{A}|=\\sum_{1 \\leq i_1\u0026lt;i_2\u0026lt;\\cdots\u0026lt;i_k \\leq n} \\boldsymbol{A}\\left( \\begin{array}{cccc} i_1 \u0026amp; i_2 \u0026amp; \\cdots \u0026amp; i_k ; j_1 \u0026amp; j_2 \u0026amp; \\cdots \u0026amp; j_k \\end{array}\\right) \\widehat{\\boldsymbol{A}}\\left(\\begin{array}{cccc} i_1 \u0026amp; i_2 \u0026amp; \\cdots \u0026amp; i_k ; j_1 \u0026amp; j_2 \u0026amp; \\cdots \u0026amp; j_k \\end{array}\\right) . \\end{aligned} $$\n注 Laplace定理可以让我们以任意几行或者几列将行列式进行展开，在理论分析中有重要作用， 可以证明以下推论:\n推论5 以分块的形式表示行列式，若$|A|$为分块上三角或分块下三角行列式，有\n$$ \\begin{aligned} |A| \u0026 =\\left|\\begin{array}{ll} B \u0026 C \\\\ 0 \u0026 D \\end{array}\\right|=\\left|\\begin{array}{ll} B \u0026 0 \\\\ C \u0026 D \\end{array}\\right| =|B||D| \\end{aligned} $$ 第二章 矩阵 第三章 线性空间 第四章 线性映射 第五章 多项式 第六章 特征值 第七章 相似标准型 第八章 二次型 第九章 内积空间 第十章 双线性型 ","date":"2025-01-26T00:21:05+08:00","image":"https://example.com/post_file/%E9%AB%98%E7%AD%89%E4%BB%A3%E6%95%B0/cover.jpg","permalink":"https://example.com/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E9%AB%98%E7%AD%89%E4%BB%A3%E6%95%B0/","title":"学习笔记|高等代数"},{"content":"Course notes 这篇blog用于记录和整理北京邮电大学\u0026amp;QMUL合办专业——电信工程及管理(Telecommunications engineering and management)专业学习笔记，以及一些课程教材推荐。\n大一 高等数学\n参考书目：1) Advanced Mathematics，By Jianhua Yuan,et al\n2) Calculus, 13th Edition,by George B. Thomas, et al\n线性代数\n参考书目：1) linear algebra，by Wenbo Zhang,et al\n2) Linear Algebra and its Applications,David C.Lay,et al\n大学物理\n课程笔记：note\n参考书目：1) Physics for Scientists and Engineers with Modern Physics: D. C. Giancoli.\n2) Berkeley Physics Course\n3) 电磁学（第三版）,作者：贾起民、郑永令、陈暨耀\n4) 新概念物理学教程 光学，作者：赵凯华\n电路分析\n参考书目：电路分析基础（第二版）（英文版）作者：王宏祥；James W. Nilsson, Susan A. Riedel\nC语言程序设计基础\n大二 数字电路设计\n课程笔记:note 参考书目:Digital Design: Principles and Practices, 5th edition by John F. Wakerly\nJAVA\n课程笔记：note 参考书目：Head First Java, 3rd Edition by Kathy Sierra, Bert Bates, Trisha Gee\n概率论与随机过程\n课程笔记：note\n参考书目：概率论与随机过程,作者:周清 张丽华\n参考课程：随机过程 张颢 2024年春\n计算机网络\n课程笔记：note\n参考书目：Computer Networking: A Top-Down Approach,Authors:PictureJames F. Kurose,PictureKeith W. Ross\n数字信号处理\n课程笔记：note\n参考书目：Discrete Time Signal Processing by Alan V. Oppenheim and Ronald W. Schafer.\n参考课程：数字信号处理 张颢 2023年秋\n工程数学\n课程笔记：note\n参考书目：Engineering Mathematics，By Xia Shi\n信号与系统\n课程笔记：note\n参考书目：Signals and Systems 2nd Edition，by Alan Oppenheim , Alan Willsky , S. Nawab\nAI导论\n课程笔记：note\n强化学习基础：note\n参考课程：1) 强化学习基础 （本科生课程） 北京邮电大学 鲁鹏\n2) 机器视觉教学课程\n电子电路基础\n课程笔记：note\n参考书目：1) 新概念模拟电路，作者：杨建国\n2) 模拟电子技术基础（第五版）,作者：童诗白等\n数值计算方法\n参考书目：沈剑华,数值计算基础（第二版）\n数据结构\n课程笔记：note\n参考书目：数据结构，作者：严蔚敏\n企业管理\n课程笔记：note\n参考视频：【元首的愤怒】北邮国院大二企业管理期末考试\n大三 通信原理\n课程笔记：note\n参考书目： 1) 通信原理（第4版） 周炯磐 庞沁华 续大我 吴伟陵 杨鸿文\n2) Fundamentals of Communication Systems.John G.Proakis;Masoud Salehi\n电磁场与电磁波\n课程笔记：note\n参考书目：Electromagnetic Field Theory Fundamentals,Guru\n机器学习\n课程笔记：note\n参考书目：1) Foundations of Machine Learning,second edition,Mehryar Mohri, Afshin Rostamizadeh,and Ameet Talwalkar\n2) Understanding Machine Learning:From Theory to Algorithms,by Cambridge University Press.\n参考课程：Introduction to Machine Learning\n网络编程\n课程笔记：note\n参考书目：An Introduction to Network Programming with Java, by Jan Graba, 3rd Edition\n数字系统设计\n课程笔记：note\n参考书目：Circuit design with VHDL by Pedroni, Volnei A\n","date":"2025-01-11T00:21:05+08:00","image":"https://example.com/post_file/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/cover.png","permalink":"https://example.com/p/course-notes/","title":"Course notes"},{"content":"Lab\u0026rsquo;s group meeting on December 2.The article Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions was shared at the group meeting, see the pdf of this slide:Slide.\n","date":"2024-12-02T16:50:00+08:00","image":"https://example.com/post_file/sampling_theory/cover.png","permalink":"https://example.com/p/sampling-is-as-easy-as-learning-the-score-theory-for-diffusion-models-with-minimal-data-assumptions/","title":"Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions"},{"content":"Score-Based Variational Inference for Inverse Problems Lab\u0026rsquo;s group meeting on September 1.The article Score-Based Variational Inference for Inverse Problems was shared at the group meeting, see the pdf of this tutorial:Score-Based Variational Inference for Inverse Problems.\n","date":"2024-09-19T20:00:00+08:00","image":"https://example.com/post_file/Score-Based_Variational_Inference_for_Inverse_Problems/cover.png","permalink":"https://example.com/p/score-based-variational-inference-for-inverse-problems/","title":"Score-Based Variational Inference for Inverse Problems"},{"content":"My mathematical modeling 这篇blog用于记录和整理我参加过的数学建模比赛以及论文。\n2024年美国大学生数学竞赛(D题)，五大湖水位控制：PDF。 2024年全国大学生统计建模竞赛，山火预测与调控：PDF。 2024年北京邮电大学数学建模训练营选拔，定日镜场优化：PDF。 2024年全国大学生数学竞赛(B题)，生产过程中的决策问题:PDF。 ","date":"2024-08-17T00:21:05+08:00","image":"https://example.com/post_file/My_mathematical_modeling/cover.png","permalink":"https://example.com/p/my-mathematical-modeling/","title":"My mathematical modeling"},{"content":"Bayesian signal processing 该博客为张颢老师所讲的现代信号处理的贝叶斯信号处理部分的课程个人学习笔记,涵盖贝叶斯公式的含义,贝叶斯学派与频率学派的关系,贝叶斯统计模型的目标函数,蒙特卡洛采样和贝叶斯滤波的内容。\nIntroduce to bayes method Bayes and frequency 使用贝叶斯公式并不意味着在使用贝叶斯方法,频率学派也会经常使用贝叶斯公式,早在贝叶斯之前,贝叶斯公式就已经在Laplce等数学家中非常娴熟的应用,贝叶斯方法在思想上面的含义要远远大于数学形式上的贝叶斯公式。\n摘抄知乎上Xiangyu Wang的回答：频率学派和贝叶斯学派最大的差别其实产生于对参数空间的认知上。所谓参数空间,就是你关心的那个参数可能的取值范围。频率学派（其实就是当年的Fisher）并不关心参数空间的所有细节,他们相信数据都是在这个空间里的“某个”参数值下产生的（虽然你不知道那个值是啥）,所以他们的方法论一开始就是从“哪个值最有可能是真实值”这个角度出发的。于是就有了最大似然（maximum likelihood）以及置信区间（confidence interval）这样的东西,你从名字就可以看出来他们关心的就是我有多大把握去圈出那个唯一的真实参数。而贝叶斯学派恰恰相反,他们关心参数空间里的每一个值,因为他们觉得我们又没有上帝视角,怎么可能知道哪个值是真的呢？所以参数空间里的每个值都有可能是真实模型使用的值,区别只是概率不同而已。于是他们才会引入先验分布（prior distribution）和后验分布（posterior distribution）这样的概念来设法找出参数空间上的每个值的概率。最好诠释这种差别的例子就是想象如果你的后验分布是双峰的,频率学派的方法会去选这两个峰当中较高的那一个对应的值作为他们的最好猜测,而贝叶斯学派则会同时报告这两个值,并给出对应的概率。\n假如我们有一个统计模型$f(x,\\theta)$去估计原始数据$x$的分布,频率学派认为$\\theta$是固定的,尽管我们现在还没有办法得到,贝叶斯学派认为$\\theta$是随机的,是服从一个概率分布的随机变量。\nBayes: The Science of cognitive law 贝叶斯的观点是符合人脑的认知规律的,也是现代认知心理学的基础：在认识一件事物之前,我们会先对这个事物有一定的认知,在与事物接触的过程中,我们的认知会逐渐的发生变化,或加强我们的认知,或减弱我们的认知,我们的认知从而得到了进步,这与马原中所说的认知随着实践螺旋式上升也有异曲同工之妙。贝叶斯公式就是表述了这个认知的过程： $$ \\begin{equation} P(\\theta \\mid X)=\\frac{P(X \\mid \\theta) P(\\theta)}{P(X)} \\end{equation} $$ 贝叶斯学派通过引入三个名词对贝叶斯公式进行解释：\n名词 公式 先验分布(Prior) $P(\\theta)$ 似然(Likelihood) $P(X \\mid \\theta)$ 后验分布(Postertor) $P(\\theta \\mid X)$ 先验分布就是在接触事物数据之前我们已经有的认知,似然是在接触事物的过程中,在给定先验知识的条件下我们对数据的认知,后验分布是我们在数据的基础上对事物的认知,机器学习中的“Learning”这一概念,很大程度上就是贝叶斯方法中的使用后验分布去调整先验分布,当数据量足够大的时候,$P(\\theta)$就越来越接近真实的概率模型,另外,分母$P(X)$经常被成为贝叶斯因子。\n我们再来观察特殊的情况。如果数据与先验独立,即$X \\perp \\theta$,有$P(\\theta \\mid X)=P(\\theta)$,即先验等于后验,也就是说在接触数据的过程中,我们并没有学到有用的信息。如果$P(\\theta)=0$,我们的先验分布和后验分布会一直等于0,所以在很多机器学习算法中设置初始迭代解的时候,会避开零概率的设置。\nExample: Naive Bayes\n朴素贝叶斯是一种简单的机器学习分类器,分类任务就是在给定某些特征 $\\lbrace F_i \\rbrace_{i=1}^N$(Features)的条件下,判断某一事物的分类$C$(Catogories),即我们需要获得：$P(C \\mid F_1:F_N)$,数据是给定条件,分类是随机变量,这就是贝叶斯方法中所说的后验分布,我们可以使用先验分布和似然来表示该后验分布： $$ \\begin{equation} P(C \\mid F_1:F_N)=\\frac{P(F_1:F_N \\mid C) P(C)}{P(F_1:F_N)} \\end{equation} $$ 对不同特征求给定条件的联合分布是比较困难的,naive bayes做了一个假设,在给定分类条件下,不同特征之间相互独立,即我们可以重写上述公式为 $$ \\begin{equation} P(C \\mid F_1:F_N)=\\frac{\\prod_{i=1}^N P(F_i \\mid C) P(C)}{P(F_1:F_N)} \\end{equation} $$ 分母可以通过统计样本得出,但是并不重要,因为所有的后验分布共享一个分母,当分类器判别样本时,都具有相同的权重。\nExample:Use bayes to process Gaussian distribution\n联想DDPM中求$q(x_{t-1} \\mid x_t,x_0)$的过程,我们使用贝叶斯公式去计算似然和先验都为Gausssian分布的情况,即$\\lbrace X_1,\u0026hellip;,X_N \\rbrace \\overset{\\mathrm{iid}}{\\sim}N(\\mu,\\sigma_x^2)$,$\\mu\\sim N(\\mu_\\theta,\\sigma_\\mu^2)$,已知先验分布和似然,去求后验分布,即\n$$ \\begin{equation} \\begin{aligned} \u0026 P\\left(\\mu \\mid x_1: x_N\\right)=\\frac{P\\left(x_1: x_N \\mid \\mu\\right) P(\\mu)}{P\\left(x_1: x_N\\right)} \\\\ \u0026 \\propto \\frac{\\exp \\left\\{-\\frac{1}{2 \\sigma_k^2} \\sum_{k=1}^N\\left(x_k-\\mu_x\\right)^2\\right\\} \\exp \\left\\{-\\frac{1}{2 \\sigma_\\mu^2}\\left(\\mu-\\mu_\\theta\\right)^2\\right\\}}{\\int_{-\\infty}^{+\\infty} \\exp \\left\\{-\\frac{1}{2 \\sigma_x^2} \\sum_{k=1}^N\\left(x_k-\\mu_x\\right)^2-\\frac{1}{2 \\sigma_\\mu^2}\\left(\\mu-\\mu_\\theta \\right)^2\\right\\} d \\mu} \\end{aligned} \\end{equation} $$ 拿出分母中指数内的部分(其实也是分子部分)：\n$$ \\begin{equation} \\begin{aligned} \u0026 -\\frac{1}{2 \\sigma_x^2} \\sum_{k=1}^N\\left(x_k-\\mu_x\\right)^2-\\frac{1}{2 \\sigma_\\mu^2}\\left(\\mu-\\mu_\\theta \\right)^2 \\\\ = \u0026 -\\frac{1}{2 \\sigma_x^2}\\left(\\sum_{k=1}^N x_k^2+N \\mu_x^2- \\mu_x \\sum_{k=1}^N x_k\\right)-\\frac{1}{2 \\sigma_\\mu^2}\\left(\\mu^2+\\mu_\\theta^2-2 \\mu_\\theta \\mu\\right) \\\\ = \u0026 \\left(-\\frac{1}{2 \\sigma_x^2}-\\frac{1}{2 \\sigma_\\mu^2}\\right) \\mu^2+\\left(-\\frac{-2 \\sum_{k=1}^N x_k}{2 \\sigma_x^2}-\\frac{-2 \\mu_\\theta}{2 \\sigma_\\mu^2}\\right) \\mu+Q\\left(x_1:x_N\\right) \\\\ = \u0026 -\\frac{1}{2}\\left(\\frac{n}{\\sigma_x^2}+\\frac{1}{\\sigma_\\mu^2}\\right)\\left(\\mu-\\left(\\frac{\\sum_{k=1}^N x_k}{\\sigma_x^2}+\\frac{\\mu_\\theta}{\\sigma_\\mu^2}\\right)\\right)^2+Q^{\\prime}\\left(x_1:x_N\\right) \\end{aligned} \\end{equation} $$ 余项$Q(x_1:x_N)$在分子与分母中都有,且与随机变量$\\mu$无关,所以原式可以写做正比于分母也是Gaussian分布的形式： $$ \\begin{equation} \\begin{aligned} \u0026P\\left(\\mu \\mid x_1: x_N\\right)\\\\ \u0026\\propto \\frac{\\exp \\left\\{-\\frac{1}{2 \\sigma_k^2} \\sum_{k=1}^N\\left(x_k-\\mu_x\\right)^2\\right\\} \\exp \\left\\{-\\frac{1}{2 \\sigma_\\mu^2}\\left(\\mu-\\mu_\\theta\\right)^2\\right\\}}{\\int_{-\\infty}^{+\\infty} \\exp \\left\\{-\\frac{1}{2 \\sigma_x^2} \\sum_{k=1}^N\\left(x_k-\\mu_x\\right)^2-\\frac{1}{2 \\sigma_\\mu^2}\\left(\\mu-\\mu_\\theta \\right)^2\\right\\} d \\mu} \\\\ \u0026\\propto \\frac{\\exp \\left\\{ -\\frac{1}{2}\\left(\\frac{n}{\\sigma_x^2}+\\frac{1}{\\sigma_\\mu^2}\\right)\\left(\\mu-\\left(\\frac{\\sum_{k=1}^N x_k}{\\sigma_x^2}+\\frac{\\mu_\\theta}{\\sigma_\\mu^2}\\right)\\right)\\mathbf{/}\\left(\\frac{n}{\\sigma_x^2}+\\frac{1}{\\sigma_\\mu^2} \\right)^2\\right\\}}{\\int_{-\\infty}^{+\\infty}\\exp \\left\\{ -\\frac{1}{2}\\left(\\frac{n}{\\sigma_x^2}+\\frac{1}{\\sigma_\\mu^2}\\right)\\left(\\mu-\\left(\\frac{\\sum_{k=1}^N x_k}{\\sigma_x^2}+\\frac{\\mu_\\theta}{\\sigma_\\mu^2}\\right)\\mathbf{/}\\left(\\frac{n}{\\sigma_x^2}+\\frac{1}{\\sigma_\\mu^2} \\right)\\right)^2\\right\\}d \\mu} \\end{aligned} \\end{equation} $$ 分母在积分过后与$\\mu$无关,可以看作一个加权,综合所有的加权,只需注意分子中Gaussian分布的形式即可,那么我们可以得到： $$ P\\left(\\mu \\mid x_1: x_N\\right)\\sim N\\left(\\left(\\frac{\\sum_{k=1}^N x_k}{\\sigma_x^2}+\\frac{\\mu_\\theta}{\\sigma_\\mu^2}\\right)\\mathbf{/}\\left(\\frac{n}{\\sigma_x^2}+\\frac{1}{\\sigma_\\mu^2} \\right),\\frac{\\sigma_x^2\\sigma_\\mu^2}{N\\sigma_\\mu^2+\\sigma_x^2}\\right) $$ 令$\\bar{X} = \\frac{1}{N}\\sum_{k=1}^N x_k$,可以将后验分布的均值写为 $$ \\mu =\\left(\\frac{\\frac{n}{\\sigma _x^2}}{\\frac{n}{\\sigma_x^2}+\\frac{1}{\\sigma_\\mu^2}}\\right) \\bar{X}+\\left(\\frac{\\frac{1}{\\sigma_\\mu^2}}{\\frac{n}{\\sigma_x^2}+\\frac{1}{\\sigma_\\mu^2}}\\right) \\mu_\\theta $$ 可见,后验分布可以当作某种似然与先验分布的线性凸组合,当$n$足够大时,数据的加权十分大,就可以忘记先验,当$\\sigma_x$足够大时,数据的信任度很低,学习的权重会尽可能的减少,从而保持先验,其实这种似然和先验分布都是Guassian分布的情况下就是简化的线性高斯模型,可以有更统一的矩阵描述形式。 Statistical decision theory 贝叶斯方法的核心观点就是利用后验分布去学习统计模型$\\hat{\\theta}(x)$,去对先验分布进行优化,那么就需要我们定义一种度量$d(\\hat{\\theta}(x),\\theta)$,从而通过最小化这种度量的方式完成先验分布的优化,$d(\\hat{\\theta}(x),\\theta)$是随机的,所以我们希望消除他的随机性求平均距离,即我们的目标为\n$$ \\begin{equation} \\hat{\\theta}(x) = \\underset{\\hat{\\theta}}{\\arg \\min }\\mathbb{E}_{x,\\theta}[d(\\hat{\\theta}(x),\\theta) ] \\end{equation} $$ 平均距离$\\mathbb{E}_{x,\\theta}[d(\\hat{\\theta}(x),\\theta) ]$ 一般被成为风险(Risk),该目标函数也被称为风险极小化,频率学派和贝叶斯学派对该目标函数有不同方式的处理：\n$$ \\begin{equation} \\begin{aligned} E[d(\\hat{\\theta}(x), \\theta)]\u0026=\\iint d(\\theta, \\hat{\\theta}(x)) P_{x, \\theta}\\left(x, \\theta\\right) d x d\\theta\\\\ \u0026=\\iint d(\\theta, \\hat{\\theta}(x)) P_{x\\mid \\theta}\\left(x\\mid \\theta\\right) d xP(\\theta) d\\theta\\\\ \u0026=\\iint d(\\theta, \\hat{\\theta}(x)) P_{x\\mid \\theta}\\left(\\theta \\mid x\\right) d \\theta P(x) dx\\\\ \\end{aligned} \\end{equation} $$ 可见,频率学派尽可能的削弱$\\theta$的随机性,因为他们认为$\\theta$是确定的（所以在频率学派中有极大化似然这种方法）,但是频率学派种所需要的似然往往是不可以直接计算的,因此有了经验风险极小化(Empirical Risk Minimization,ERM)理论,该理论一般将样本均值取代理论均值,即 $$ E[d(\\hat{\\theta}(x), \\theta)]=\\frac{1}{N}\\sum_{k=1}^Nd(\\theta_k, \\hat{\\theta}_k(x)) $$ 贝叶斯学派则是尽可能削弱数据的随机性,认为已经发生的实验就是确定的,将问题的重心转化到后验分布的计算上,在后面我们会看到,使用贝叶斯方法处理统计模型的关键就是得到后验分布以及如何处理后验分布。\nChoice of measurement MSE\n选择度量为均方误差,可以定义风险(忽略了数据的随机性)： $$ \\begin{equation} R(\\theta, \\hat{\\theta})=\\int(\\theta-\\hat{\\theta})^2 P(\\theta \\mid x) d \\theta \\end{equation} $$ 由拉格朗日乘子法,最优的$\\hat{\\theta}$在风险函数的导数为零处,即使得$\\nabla_{\\hat{\\theta}} R(\\theta, \\hat{\\theta})=0$,我们求解该式,可以得到 $$ \\begin{equation} \\hat{\\theta}^*=\\frac{\\int \\theta p(\\theta \\mid x) d \\theta}{\\int p(\\theta \\mid x) d \\theta}=\\int \\theta p(\\theta \\mid x) d \\theta=E[\\theta \\mid x] \\end{equation} $$ 这与频率学派下的MMSE最优解不谋而合,都是均方误差的形式,但是各自的随机变量不同,由此可见,条件期望就是均方误差意义下的最优估计。\nMAE\n平均绝对误差意义下,风险函数为 $$ \\begin{equation} R(\\theta, \\hat{\\theta})=\\int\\mid \\theta-\\hat{\\theta}\\mid P(\\theta \\mid x) d \\theta \\end{equation} $$ 由于风险函数存在绝对值,所以无法直接求导,我们可以将绝对值拆开写为 $$ \\begin{equation} R(\\theta, \\hat{\\theta})=-\\int_{-\\infin}^{\\hat{\\theta}}( \\theta-\\hat{\\theta}) P(\\theta \\mid x) d \\theta+\\int_{\\hat{\\theta}}^{+\\infin}(\\theta-\\hat{\\theta}) P(\\theta \\mid x) d \\theta \\end{equation} $$ 现在我们就可以利用变上下限积分的技巧对风险函数进行求导,求导的过程就是把$\\theta-\\hat{\\theta}$拆开,使得积分的变上限不在积分变量之内,在利用求导法则正常求导即可,可以求得： $$ \\nabla_{\\hat{\\theta}} R(\\theta, \\hat{\\theta})=\\int_{-\\infty}^{\\hat{\\theta}} p(\\theta \\mid x) d \\theta-\\int_{\\hat{\\theta}}^{+\\infty} p(\\theta \\mid x) d \\theta=0 $$ 直观的观察这个式子,当风险函数的导数等于0时,$\\hat{\\theta}$就是平分后验分布概率密度函数面积的垂直于$\\theta$轴的直线,也就是统计量中的中位数,即中位数是MAE意义下的最优估计。\nMAP\n这里的MAP指极大化后验概率的意思,定义度量为：\n$$ \\begin{equation} d(\\theta, \\hat{\\theta})=\\left\\{\\begin{array}{l} 1,|\\hat{\\theta}-\\theta|\u003e\\delta \\\\ 0,|\\hat{\\theta}-\\theta| \\leq \\delta \\end{array}\\right. \\end{equation} $$ 其中,$\\delta$是我们主观挑选的误差容忍数值,当$\\delta$足够小时,风险函数可以写为：\n$$ \\begin{equation} \\begin{aligned} R(\\theta, \\hat{\\theta}) \u0026 =\\int d(\\theta, \\hat{\\theta}) p(\\theta, x) d \\theta \\\\ \u0026 =\\int_{|\\hat{\\theta}-\\theta|\u003e\\delta} p(\\theta \\mid x) d \\theta \\\\ \u0026 =1-\\int_{|\\hat{\\theta}-\\theta| \\leqslant \\delta} p(\\theta \\mid x) d \\theta \\\\ \u0026 \\approx 1-2 \\delta p(\\theta \\mid x) \\end{aligned} \\end{equation} $$ 那么,我们就可以等价的将最小化风险写为最大化后验概率,即 $$ \\begin{equation} \\underset{\\hat{\\theta}}{\\arg \\min }R(\\theta,\\hat{\\theta})\\iff\\underset{\\hat{\\theta}}{\\arg \\max }p(\\theta \\mid x) \\end{equation} $$ 用统计量来表述的话就是最大化众数。 Example:Use bayes to process Bernoulli distribution\n假设我们有先验分布$\\theta$,似然$X\\sim B(n,\\theta)$,目标计算$\\theta$的后验分布以及在均方意义下的最优估计。\n由Bayes公式,有\n$$ \\begin{equation} \\begin{aligned} P(\\theta \\mid x) \u0026 =\\frac{P(x \\mid \\theta) P(\\theta)}{P(x)} \\\\ \u0026 =\\frac{P(x \\mid \\theta) P(\\theta)}{\\int P(x, \\theta) d \\theta} \\end{aligned} \\end{equation} $$ 接下来,我们考虑当先验分布服从标准均匀分布的情况下后验概率的计算,即计算 $$ \\begin{equation} P(\\theta \\mid x)=\\frac{\\theta^x(1-\\theta)^{n-x}}{\\int_0^1 \\theta^x(1-\\theta)^{n-x} d \\theta} \\end{equation} $$ 在计算后验概率之前,我们需要引入Beta分布来辅助我们的贝叶斯因子的计算,Beta分布的pdf为\n$$ \\begin{equation} \\begin{aligned} f(x ; \\alpha, \\beta) \u0026 =\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\int_0^1 u^{\\alpha-1}(1-u)^{\\beta-1} d u} \\\\ \u0026 =\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} x^{\\alpha-1}(1-x)^{\\beta-1} \\\\ \u0026 =\\frac{1}{\\mathrm{~B}(\\alpha, \\beta)} x^{\\alpha-1}(1-x)^{\\beta-1} \\end{aligned} \\end{equation} $$ 其中,$\\alpha,\\beta\u0026gt;0$,$\\Gamma(x)$是Gamma函数,有性质 $$ \\Gamma(x)=\\int_0^{+\\infty} t^{x-1} e^{-t} d t(x\u0026gt;0)=(x-1)! $$ Beta函数$B(\\alpha, \\beta)$的定义为$B(\\alpha, \\beta)=\\int_0^1 t^{\\alpha-1}(1-t)^{\\beta-1} \\mathbf{d} t$,现在我们来证明$\\mathrm{~B}(\\alpha, \\beta)=\\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$:\n观察Beta函数的定义,类似卷积的定义,注意到：\n$$ \\begin{aligned} B(\\alpha, \\beta)\u0026=\\int_0^1 t^{\\alpha-1}(x-t)^{\\beta-1} \\mathbf{d} t\\mid_{x=1} \\\\\u0026=f(x)*g(x)\\mid_{x=1} \\end{aligned} $$ 在上述公式中,我们定义了两个幂函数$f(x): =x^{\\alpha-1},g(x):=x^{\\beta-1},$这两个幂函数的支撑集应该是[0,1],这样才能满足积分的上下限要求,现在对两个幂级数的卷积进行Laplce变换：\n$$ \\mathcal{L}\\left\\{x^{\\alpha-1} * x^{\\beta-1}\\right\\}=\\mathcal{L}\\left\\{x^{\\alpha-1}\\right\\} \\cdot \\mathcal{L}\\left\\{x^{\\beta-1}\\right\\} $$ 而幂级数的Laplace变换可以和Gamma函数联系起来,即\n$$ \\begin{equation} \\begin{aligned} \\mathcal{L}\\left\\{t^a\\right\\}\u0026=\\int_0^{\\infty} t^a e^{-s t} d t \\\\ \u0026=\\frac{1}{s^{a+1}} \\int_0^{\\infty}(s t)^a e^{-(s t)}(s d t) \\\\ \u0026=\\frac{1}{s^{a+1}} \\int_0^{\\infty} \\tau^{(a+1)-1} e^{-\\tau} d \\tau \\\\ \u0026=\\frac{\\Gamma(a+1)}{s^{a+1}} \\end{aligned} \\end{equation}$$ 继续我们上述的Laplace变换,有\n$$ \\begin{equation} \\begin{aligned} \\mathcal{L}\\left\\{x^{\\alpha-1}\\right\\} \\cdot \\mathcal{L}\\left\\{x^{\\beta-1}\\right\\}\u0026=\\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{s^{\\alpha+\\beta}} \\\\ \u0026=\\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\frac{\\Gamma(\\alpha+\\beta-1+1)}{s^{(\\alpha+\\beta-1)+1}}\\\\ \u0026=\\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\mathcal{L}\\left\\{x^{\\alpha+\\beta-1}\\right\\} \\end{aligned} \\end{equation} $$ 这时令$x=1$,便可以得到$\\mathrm{~B}(\\alpha, \\beta)=\\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$,证毕。\n现在我们使用Beta函数来计算后验分布,注意,如果令$\\alpha = x+1,\\beta = n-x+1$,后验分布的贝叶斯因子为$B(\\alpha,\\beta)=B(x+1,n-x+1)$,那么有\n$$ \\begin{equation} \\begin{aligned} \\int_0^1 \\theta^x(1-\\theta)^{n-x} d \\theta \u0026 =B(x+1, n-x+1) \\\\ \u0026 =\\frac{\\Gamma(x+1) \\Gamma(n-x+1)}{\\Gamma(n+2)} \\\\ \u0026 =\\frac{x!(n-x)!}{(n+1)!} \\end{aligned} \\end{equation} $$ 则我们计算出来的后验分布最终形式为 $$ \\begin{equation} P(\\theta, x)=\\frac{(n+1)!}{x!(n-x)!} \\theta^x(1-\\theta)^{n-x} \\end{equation} $$ 可见该后验分布实际上是服从Beta分布的。根据MSE意义下的最优估计,有\n$$ \\begin{equation} \\begin{aligned} E[\\theta \\mid x] \u0026 =\\int_0^1 \\theta \\frac{(n+1)!}{x!(n-x)!} \\theta^x(1-\\theta)^{n-x} d \\theta \\\\ \u0026 =\\frac{(n+1)!}{x!(n-x)!} \\int_0^1 \\theta^{x+1}(1-\\theta)^{n-x} d \\theta \\\\ \u0026 =\\frac{(n+1)!}{x!(n-x)!} B(x+2, n-x+1) \\\\ \u0026 =\\frac{(n+1)!}{n!(n-x)!} \\frac{(x+1)!(n-x)!}{(n+2)!} \\\\ \u0026 =\\frac{x+1}{n+2} \\end{aligned} \\end{equation} $$ 至此,我们求得了后验分布和MSE意义下的模型最优估计。 如果再进一步的处理该条件期望,我们可以得到线型凸组合的形式： $$ E[\\theta \\mid x]=\\frac{n}{n+2}\\frac{x}{n}+\\frac{2}{n+2}\\frac{1}{2} $$ $\\frac{x}{n}$反应了似然意义上的某种均值,$\\frac{1}{2}$为先验分布的均值,这也显示了在伯努利实验的情况下,最优估计的过程是不断线性凸组合的过程。\nExample:Conjugate prior\n在上个例子中,我们一开始假设先验分布服从标准均匀分布,但是我们根据伯努利的似然求出后验分布之后,发现后验分布并不服从标准均匀分布,而是服从Beta分布,这说明我们的先验分布并没有反映事物的客观性质,在统计学中,如果我们的先验分布和后验分布的分布类型一致,我们就称该先验分布为共轭先验。\n在一些比较简单的概率模型中,共轭先验是已知的并且可以提前制定的,例如当似然服从伯努利分布时,共轭先验为Beta分布,但是更多的(绝大多数的)概率模型的共轭先验是难以得到的。我们沿用上一个例子的情景,令先验分布为Beta分布,即 $$ P(\\theta)=\\frac{(a+b+1)!}{(a-1)!(b-1)!}\\theta^{a-1}(1-\\theta)^{b-1} $$ 使用与上一个例子同样的方法去求后验分布和条件期望,最终可以求得条件期望为 $$ E[\\theta \\mid x] = \\frac{x+a}{n+a+b} $$ 同样的,该条件期望也可以写成线型图组合的形式,即 $$ E[\\theta \\mid x]=\\frac{a+b}{n+a+b}\\frac{a}{a+b}+\\frac{n}{n+a+b}\\frac{x}{n} $$ 依然是某种实验中的均值与先验分布均值的线性凸组合,从我们在上一个章节中的Gauss分布似然到这个章节的伯努利分布似然,最后都得到了线性凸组合的学习形式,这可能并不是一种偶然,这些简单的例子给我们启发：在高维空间中,复杂分布的学习过程也可能是迭代的线性凸组合。\nSampling 使用Bayes方法的核心在于后验分布的处理，但是后验分布的形式往往是极其复杂的，极大多数的后验分布都是难以去积分甚至不可积的，统计中常用的方法就是生成符合后验分布的样本，从而利用样本代替解析表达的分布进行计算，这种生成样本的方法又被称作采样。\n模拟是指把某一现实的或抽象的系统的某种特征或部分状态， 用另一系统（称为模拟模型）来代替或近似。 为了解决某问题， 把它变成一个概率模型的求解问题， 然后产生符合模型的大量随机数， 对产生的随机数进行分析从而求解问题， 这种方法叫做随机模拟方法， 又称为蒙特卡洛(Monte Carlo)方法。\nInverse distribution function 逆分布采样的描述很简单，目标生成随机变量$X\\sim F(X)$的样本($F(x)$为累计分布函数)，只需要生成随机样本$Y\\sim F^{-1}(U)$即可，$Y$即想要得到的分布，$U$为服从标准正态分布的随机变量。\n这样来看，生成随机变量的样本的过程就很简单，第一步，生成均匀分布的样本(可以利用线性同余法等方法生成)，第二步，求得目标分布累积分布函数的逆，第三步，代入累积分布函数的逆求得目标样本。\n现在简单的证明一下随机变量$Y$与随机变量$X$服从同一个分布，求$Y$的累积分布函数： $$ \\begin{equation} \\begin{aligned} F_Y(y)\u0026amp; =P(y\\leq Y)\\ \u0026amp; =P(F^{-1}(u)\\leq Y)\\ \u0026amp; =P(u\\leq F(Y))\\ \u0026amp; =F(Y) \\end{aligned} \\end{equation} $$ 可见，$Y$的累积分布函数实际上就是$X$的累积分布函数，所以它们两个是同一个随机变量，如此，对于易于求得逆累积分布的随机变量，我们便可以通过这种方法求得符合该分布的样本，例如指数分布、泊松分布。\nExample:Sampling for Gaussian Distribution Guassian分布的逆分布函数并不好求，虽然我们可以通过中心极限定理生成Gaussian分布，但那种方法并不属于逆分布生成，Box-Muller算法是一种经典的Gaussian分布生成算法，使用了极坐标变换来处理Gaussian分布难以求逆的问题，接下来我们介绍这种方法。\n假设我们目标求得两个独立同分布的标准高斯随机变量，$X_1,X_2\\overset{\\mathrm{iid}}{\\sim} N(0,1)$，则它们的联合概率密度为 $$ \\begin{equation} \\begin{aligned} f_{X Y}(x, y) \u0026amp; =\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{x^2}{2 \\sigma^2}\\right) \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{y^2}{2 \\sigma^2}\\right) \\ \u0026amp; =\\frac{1}{2 \\pi } \\exp \\left(-\\frac{x^2+y^2}{2 }\\right) \\end{aligned} \\end{equation} $$ 对该联合概率密度进行极坐标变换，有 $$ \\begin{equation} \\begin{aligned} f_{X Y}(x, y) \u0026amp;=f_{XY}(\\rho \\cos\\theta,\\rho \\sin\\theta)\\ \u0026amp;=\\frac{1}{2 \\pi } \\exp \\left(-\\frac{\\rho^2}{2 }\\right)\\rho \\end{aligned} \\end{equation} $$\nAccept-reject sampling Importance sampling Markov chain Monte Carlo(MCMC) ","date":"2024-08-14T20:00:00+08:00","image":"https://example.com/post_file/Bayesian_signal_processing/cover.png","permalink":"https://example.com/p/bayesian-signal-processing/","title":"Bayesian signal processing"},{"content":"Correlation operation and random signal processing My midterm paper on probability theory and stochastic processes. This paper describes the concept of correlation operation, mainly introduces the application of correlation operation in random signal processing, including stationary random signal, blind source separation, template matching filter. You can read my course papers in this link:Correlation operation and random signal processing.\n","date":"2024-08-07T23:30:00+08:00","image":"https://example.com/post_file/Correlation_operation_and_random_signal_processing/cover.png","permalink":"https://example.com/p/correlation-operation-and-random-signal-processing/","title":"Correlation operation and random signal processing"},{"content":"A non-uniform node finite difference algorithm Final design of calculation method course in the first semester of sophomore year. Based on the finite difference method , I proposed an ODE algorithm of non-uniform finite difference , and verified by Matlab simulation, the accuracy of the solution is improved by 10% . You can find this course paper in this pdf : A non-uniform node finite difference algorithm.\nIn addition , I made an academic poster for a simulated academic conference in the college.You can find in this pdf : Poster.\n","date":"2024-08-07T00:22:00+08:00","image":"https://example.com/post_file/non_uni_ode/cover1.png","permalink":"https://example.com/p/a-non-uniform-node-finite-difference-algorithm/","title":"A non-uniform node finite difference algorithm"},{"content":"SMLD and SDE for diffusion model Lab\u0026rsquo;s group meeting on August 1 . I summarized two articles about diffusion model by Song Yang:SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS and Generative Modeling by Estimating Gradients of the Data Distribution . For detailed information, see the pdf of this tutorial:SMLD and SDE.\n","date":"2024-08-06T16:50:00+08:00","image":"https://example.com/post_file/SMLD_and_SDE/cover1.png","permalink":"https://example.com/p/smld-and-sde-for-diffusion-model/","title":"SMLD and SDE for diffusion model"},{"content":"Tutorial of Markdown Basic grammar 引用\n列表 列表 列表 列表 有序列表 1 2 嵌套 嵌套 TodoList a b c 表格 左对齐 右对齐 居中对齐 a b c 段落 分隔线 字体 表格 代码 斜体 * * ==高亮== == == 粗体 ** ** 斜粗体 *** *** 删除 ~~ ~~ 下划线 \u0026lt;u\u0026gt;\u0026lt;/u\u0026gt; 脚注1 代码操作 printf(\u0026quot;hello world!\u0026quot;)\n1 2 code block hello 超链接 github\n图片插入 图床：https://imgse.com/\n直接在图床里粘贴,有链接： 数学公式\n与latex一致\n$ q\\left(x_{t-1} \\mid x_t\\right)=\\frac{q\\left(x_t \\mid x_{t-1}\\right) q\\left(x_{t-1}\\right)}{q\\left(x_t\\right)} $ $$ \\begin{equation} q\\left(x_{t-1} \\mid x_t\\right)=\\frac{q\\left(x_t \\mid x_{t-1}\\right) q\\left(x_{t-1}\\right)}{q\\left(x_t\\right)} \\end{equation} $$\n脚注内容\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-08-05T12:14:10+08:00","image":"https://example.com/post_file/Markdown/cover.png","permalink":"https://example.com/p/tutorial-of-markdown/","title":"Tutorial of Markdown"}]